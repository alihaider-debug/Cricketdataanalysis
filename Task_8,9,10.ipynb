{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alihaider-debug/Cricketdataanalysis/blob/main/Task8,9,10.ipynb)"
      ],
      "metadata": {
        "id": "IwAKPGMJyYnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üß™ Step 1: Load Dataset and Preprocess**"
      ],
      "metadata": {
        "id": "psM0ZRoXn0ur"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OqOGJirk8SI",
        "outputId": "296735a7-181f-4592-9fdd-5cfbededf8cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-848b29c5ffa0>:10: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(github_url)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
            "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
            "  T = new_sum / new_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
            "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold, LeaveOneOut\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "github_url = \"https://github.com/alihaider-debug/Cricketdataanalysis/raw/main/ODI_Match_Data.csv\"\n",
        "df = pd.read_csv(github_url)\n",
        "\n",
        "# Handle categorical attributes (Label Encoding)\n",
        "label_encoders = {}\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    # Check if the column contains mixed types\n",
        "    if df[col].apply(type).nunique() > 1:\n",
        "        # Convert all values to strings if mixed types are present\n",
        "        df[col] = df[col].astype(str)\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Handle missing values (fill with median)\n",
        "df.fillna(df.median(numeric_only=True), inplace=True)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=['runs_off_bat'])\n",
        "y = df['runs_off_bat']\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìä Step 2: Holdout Validation (80-20 Split)**"
      ],
      "metadata": {
        "id": "Zk7l--GHnqbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Holdout Validation (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"üéØ Holdout Validation Results:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkG8TLB1nLt9",
        "outputId": "3b9968b0-08e3-4961-8b4c-d86b48134cc0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Holdout Validation Results:\n",
            "Accuracy: 0.4821\n",
            "RMSE: 1.7176\n",
            "F1-score: 0.4739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìà Step 3: K-Fold Cross-Validation (k=5)**"
      ],
      "metadata": {
        "id": "72bXqRweoJgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Fold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_accuracy = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
        "cv_rmse = np.sqrt(-cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error'))\n",
        "cv_f1 = cross_val_score(model, X, y, cv=kf, scoring='f1_weighted')\n",
        "\n",
        "print(\"\\nüìä K-Fold Cross-Validation Results (5 Folds):\")\n",
        "print(f\"Accuracy: {cv_accuracy.mean():.4f}\")\n",
        "print(f\"RMSE: {cv_rmse.mean():.4f}\")\n",
        "print(f\"F1-score: {cv_f1.mean():.4f}\")\n"
      ],
      "metadata": {
        "id": "6nENfRYEnWA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Leave-One-Out Cross-Validation (LOOCV)**"
      ],
      "metadata": {
        "id": "AmqOm-wRoHHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Leave-One-Out Cross-Validation (LOOCV)\n",
        "loo = LeaveOneOut()\n",
        "loo_accuracy = cross_val_score(model, X, y, cv=loo, scoring='accuracy')\n",
        "loo_rmse = np.sqrt(-cross_val_score(model, X, y, cv=loo, scoring='neg_mean_squared_error'))\n",
        "loo_f1 = cross_val_score(model, X, y, cv=loo, scoring='f1_weighted')\n",
        "\n",
        "print(\"\\nüß™ Leave-One-Out Cross-Validation (LOOCV) Results:\")\n",
        "print(f\"Accuracy: {loo_accuracy.mean():.4f}\")\n",
        "print(f\"RMSE: {loo_rmse.mean():.4f}\")\n",
        "print(f\"F1-score: {loo_f1.mean():.4f}\")\n"
      ],
      "metadata": {
        "id": "Tc_kayg4nbDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**üìù Bias-Variance Tradeoff:**\n",
        "\n",
        "Holdout: Simple and fast but higher variance due to limited training data.\n",
        "\n",
        "K-Fold: Balanced bias-variance tradeoff. Preferred for most projects.\n",
        "\n",
        "LOOCV: Low bias but very high variance and computationally expensive.\n",
        "\n",
        "\n",
        "**‚úÖ Recommendation:** Use K-Fold Cross-Validation (k=5) for the best balance of performance and efficiency."
      ],
      "metadata": {
        "id": "gM_DpmV-oTl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**----------------------------TASK 9------------------------------**"
      ],
      "metadata": {
        "id": "vhB0MXnTql5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Import Libraries and Load Data**"
      ],
      "metadata": {
        "id": "tv61Chsrq5am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "github_url = \"https://github.com/alihaider-debug/Cricketdataanalysis/raw/main/ODI_Match_Data.csv\"\n",
        "df = pd.read_csv(github_url)\n",
        "\n",
        "# Display basic info\n",
        "print(df.info())\n",
        "print(df['target'].value_counts())\n"
      ],
      "metadata": {
        "id": "rRy48UB3q-U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìä Step 2: Perform Stratified Sampling**"
      ],
      "metadata": {
        "id": "7xT_gCAOrH7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform stratified split (80% train, 20% test) based on target class\n",
        "train_set, test_set = train_test_split(df, test_size=0.2, stratify=df['target'], random_state=42)\n",
        "\n",
        "# Verify distribution in train and test sets\n",
        "print(\"Train set distribution:\\n\", train_set['target'].value_counts(normalize=True))\n",
        "print(\"\\nTest set distribution:\\n\", test_set['target'].value_counts(normalize=True))\n"
      ],
      "metadata": {
        "id": "O1uGLlTJrNVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìà Step 3: Marginal Probability Analysis**"
      ],
      "metadata": {
        "id": "qPX8Y5PzrP6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate marginal probabilities in the original, train, and test sets\n",
        "original_probs = df['target'].value_counts(normalize=True)\n",
        "train_probs = train_set['target'].value_counts(normalize=True)\n",
        "test_probs = test_set['target'].value_counts(normalize=True)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nMarginal Probabilities Comparison:\")\n",
        "print(\"Original Distribution:\\n\", original_probs)\n",
        "print(\"Train Distribution:\\n\", train_probs)\n",
        "print(\"Test Distribution:\\n\", test_probs)\n"
      ],
      "metadata": {
        "id": "ZGzsODUurUT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìù Step 4: Conclusion**\n",
        "\n",
        "‚úÖ Stratified sampling ensures that both train and test sets have the same class distribution as the original dataset.\n",
        "\n",
        "‚úÖ Marginal probabilities are consistent across sets, making the model evaluation more reliable."
      ],
      "metadata": {
        "id": "p1aC3jjirYkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**-----------------------TASK 10-------------------------**"
      ],
      "metadata": {
        "id": "mb_CktlVrjBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Step 1: Import Libraries and Load Data**"
      ],
      "metadata": {
        "id": "GsH9dOJ-rpyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load dataset (replace 'your_dataset.csv' with your file)\n",
        "df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# View first rows\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "fDwrL--drv99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Part 1: Handling Categorical Attributes"
      ],
      "metadata": {
        "id": "dCZ_t0YXrzFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. Check for Missing Values and Fill Them"
      ],
      "metadata": {
        "id": "UKDVRkOlr5uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "print(\"Missing values in categorical columns:\\n\", df[categorical_cols].isnull().sum())\n",
        "\n",
        "# Fill missing values with mode\n",
        "for col in categorical_cols:\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)\n"
      ],
      "metadata": {
        "id": "HvkDucmZsAGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Encode Categorical Features:**\n",
        "\n",
        "Label Encoding for ordinal features.\n",
        "\n",
        "One-Hot Encoding for nominal features"
      ],
      "metadata": {
        "id": "T1jDutigsDE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoding for ordinal features (e.g., Education Level: High School < Bachelor < Master)\n",
        "label_encoder = LabelEncoder()\n",
        "df['education_level'] = label_encoder.fit_transform(df['education_level'])\n",
        "\n",
        "# One-Hot Encoding for nominal features (e.g., Gender, Region)\n",
        "df = pd.get_dummies(df, columns=['gender', 'region'], drop_first=True)\n"
      ],
      "metadata": {
        "id": "e9qDUJvTsQTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2: Handling Text Attributes**"
      ],
      "metadata": {
        "id": "vR5P5D87sS0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Text Preprocessing (Cleaning)**"
      ],
      "metadata": {
        "id": "gmQPNUM7sYZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = text.lower().strip()       # Convert to lowercase\n",
        "    return text\n",
        "\n",
        "# Clean 'review' text column\n",
        "df['review'] = df['review'].apply(clean_text)\n"
      ],
      "metadata": {
        "id": "ODEzNgplscyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Convert Text to Numerical Format (TF-IDF Vectorization)**"
      ],
      "metadata": {
        "id": "Ua58_POVsfo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "tfidf_matrix = tfidf.fit_transform(df['review'])\n",
        "\n",
        "# Convert TF-IDF matrix to DataFrame and concatenate\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
        "df = pd.concat([df.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Drop the original text column\n",
        "df.drop(columns=['review'], inplace=True)\n"
      ],
      "metadata": {
        "id": "Uh-_b5fGsksb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3: Scaling Numerical Features**"
      ],
      "metadata": {
        "id": "M2F6SGbwsqRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Select numerical columns\n",
        "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Apply Standardization\n",
        "scaler = StandardScaler()\n",
        "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "\n",
        "# Optional: Apply MinMax Scaling\n",
        "# minmax_scaler = MinMaxScaler()\n",
        "# df[numerical_cols] = minmax_scaler.fit_transform(df[numerical_cols])\n"
      ],
      "metadata": {
        "id": "jlQSQtMgsr2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 4: Create New Features (If Applicable)**"
      ],
      "metadata": {
        "id": "Q2nL3TfAsu2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Create a new feature combining age and income\n",
        "df['income_per_age'] = df['income'] / (df['age'] + 1)  # Avoid division by zero\n"
      ],
      "metadata": {
        "id": "nA-8YUImszit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-p5atHoWtos8"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}