{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alihaider-debug/Cricketdataanalysis/blob/main/Task6.ipynb)"
      ],
      "metadata": {
        "id": "VEohYCMjyUnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "_YkXGilFYFNE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "github_url = \"https://github.com/alihaider-debug/Cricketdataanalysis/raw/main/ODI_Match_Data.csv\"\n",
        "df = pd.read_csv(github_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3Zc7EH-YIau",
        "outputId": "c564793c-1e55-4511-9972-00b3c6b2a423"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-1be9034deca1>:2: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(github_url)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Evaluate the Best Model on Test Dataset**"
      ],
      "metadata": {
        "id": "a6MXZf0_Yn2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# üèÜ Best Model Evaluation\n",
        "# ============================\n",
        "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# üéØ Select Best Model\n",
        "best_model = rf_tuned  # Replace with best model from Task 5 (e.g., gb_tuned)\n",
        "\n",
        "# üìä Predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# üìà Regression Metrics\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"‚úÖ Final Model: Random Forest\")\n",
        "print(f\"üìä RMSE: {rmse:.4f}\")\n",
        "print(f\"üìä R¬≤ Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "91mpDiRsYr_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìä A) Residual Plot**"
      ],
      "metadata": {
        "id": "CctNwOjmZf1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# üìä Step A: Residual Plot\n",
        "# ============================\n",
        "best_model = rf_random.best_estimator_  # Choose the best-performing model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.residplot(x=y_test, y=y_pred, lowess=True, line_kws={'color': 'red'})\n",
        "plt.title('Residual Plot')\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PEIxfX-kZjAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üß© B) Feature Importance (Tree Models Only)**"
      ],
      "metadata": {
        "id": "wKAgf5KKZpev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# üß© Step B: Feature Importance\n",
        "# ============================\n",
        "importances = best_model.feature_importances_\n",
        "features = X_train.columns\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.barplot(x=importances, y=features, palette='viridis')\n",
        "plt.title('Feature Importance')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8bi5dKo-Zu8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üìä C) Final Report**"
      ],
      "metadata": {
        "id": "qV1y7LTkZzp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# üìù Step C: Final Report\n",
        "# ============================\n",
        "final_report = pd.DataFrame({\n",
        "    'Model': ['Random Forest (Tuned)', 'Gradient Boosting (Tuned)'],\n",
        "    'RMSE': [tuned_results_df.iloc[0]['RMSE'], tuned_results_df.iloc[1]['RMSE']],\n",
        "    'R¬≤ Score': [tuned_results_df.iloc[0]['R¬≤ Score'], tuned_results_df.iloc[1]['R¬≤ Score']]\n",
        "})\n",
        "\n",
        "print(\"\\nüèÜ Final Model Evaluation Report:\")\n",
        "print(final_report)\n",
        "\n",
        "# Save report to CSV\n",
        "final_report.to_csv('final_model_evaluation.csv', index=False)\n",
        "print(\"\\n‚úÖ Report saved as 'final_model_evaluation.csv'\")\n"
      ],
      "metadata": {
        "id": "iyQrg30VZ6pV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}